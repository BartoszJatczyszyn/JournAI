# üè• JournAI ‚Äì Garmin Health Data Analysis System

"JournAI" to nowa nazwa repozytorium (wcze≈õniej roboczo: Diary-AI / Journal-AI). Nazwy katalog√≥w `Diary-AI-BE/` i `Diary-AI-FE/` pozostajƒÖ tymczasowo niezmienione (stabilne ≈õcie≈ºki w skryptach). Przy ewentualnym ich refaktorze (np. na `journai-be/`, `journai-fe/`) nale≈ºy zaktualizowaƒá: 
- odwo≈Çania w Dockerfile / docker-compose
- ≈õcie≈ºki w README / DOCKER_SETUP.md / QUICK_START.md
- regu≈Çy w `.gitignore`
- komendy w dokumentacji migracji

Je≈õli chcesz ju≈º teraz zmieniƒá remote po rename w GitHub: 
```
git remote set-url origin https://github.com/<twoj-user>/JournAI.git
```
albo przez SSH: 
```
git remote set-url origin git@github.com:<twoj-user>/JournAI.git
```

Kompletny system analizy danych zdrowotnych z urzƒÖdze≈Ñ Garmin z integracjƒÖ PostgreSQL, dziennikiem osobistym i zaawansowanƒÖ analitykƒÖ AI.

## üìä Funkcjonalno≈õci

### ‚úÖ Kompletna migracja danych Garmin:
- **959,911 rekord√≥w** zdrowotnych w 9 tabelach
- **305,354 pomiar√≥w tƒôtna** co minutƒô (24/7)
- **268,844 pomiar√≥w czƒôsto≈õci oddechowej**
- **380,672 pomiar√≥w stresu**
- **1,025 aktywno≈õci sportowych** z pe≈Çnymi danymi
- **277 sesji snu** z detalowƒÖ analizƒÖ

### üìù Dziennik osobisty:
- **69 kolumn** do trackingu ≈ºycia
- Nastr√≥j, energia, stress (skale 1-5)
- Od≈ºywianie, suplementy, nawyki
- Korelacje z danymi Garmin

### üß† **NOWE! Zaawansowana Analityka AI:**
- **Korelacje wielowymiarowe** (Pearson, Spearman, Kendall)
- **Analiza klastr√≥w** - automatyczne wykrywanie wzorc√≥w zdrowotnych
- **Analiza temporalna** - wzorce tygodniowe i sezonowe
- **Analiza regeneracji** - kompleksowa ocena odzyskiwania si≈Ç
- **Analityka predykcyjna** - prognozy energii, snu i nastroju
- **Personalizowane rekomendacje** oparte na danych

### üî¨ Specjalistyczne modu≈Çy analityczne:
- **Analiza snu**: efektywno≈õƒá, timing, wp≈Çyw na wydajno≈õƒá
- **Analiza stresu**: wzorce godzinowe, triggery, regeneracja
- **Analiza aktywno≈õci**: intensywno≈õƒá, konsystencja, korelacje z regeneracjƒÖ

### üóÉÔ∏è Zorganizowane tabele:
- `garmin_daily_summaries` - dzienne podsumowania (33 kolumny)
- `garmin_activities` - aktywno≈õci sportowe (51 kolumn)
- `garmin_sleep_sessions` - sesje snu (21 kolumn)
- `daily_journal` - dziennik osobisty (69 kolumn)

## üöÄ Szybki start

1. **Konfiguracja bazy:**
   ```bash
   # Edytuj config.env z danymi PostgreSQL
   cp config.env.example config.env
   ```

2. **Instalacja zale≈ºno≈õci:**
   ```bash
   pip install -r Diary-AI-BE/requirements.txt
   ```

3. **Uruchomienie backendu:**
   ```bash
   # NOWY! Enhanced Backend z AI Analytics
   python scripts/start_enhanced_backend.py
   
   # Lub bezpo≈õrednio
   cd Diary-AI-BE/scripts && python backend_api_enhanced.py
   
   # Backend Enhanced (zalecany)
   python scripts/start_enhanced_backend.py
   ```

4. **Dashboard:**
   Otw√≥rz `Diary-AI-FE/simple_dashboard.html` w przeglƒÖdarce

### ÔøΩ Skrypty start/stop (lokalne ≈õrodowisko)
W katalogu g≈Ç√≥wnym sƒÖ dostƒôpne uproszczone skrypty:

```bash
./start_all.sh   # uruchamia backend (enhanced) i ewentualne procesy pomocnicze
./stop_all.sh    # zatrzymuje procesy backendu (wyszukujƒÖc dzia≈ÇajƒÖce PID)
```

Zastosowanie:
- Szybki restart podczas developmentu
- Pewno≈õƒá, ≈ºe nie zostanie ‚ÄûwiszƒÖcy‚Äù proces backendu rezerwujƒÖcy port 5002

Je≈õli skrypt nie ma uprawnie≈Ñ wykonywalnych:
```bash
chmod +x start_all.sh stop_all.sh
```

W ≈õrodowisku Docker zamiast tego u≈ºywaj `docker compose up` / `down`.

### ÔøΩüê≥ Alternatywa: uruchomienie przez Docker Compose

Najprostszy spos√≥b aby ka≈ºdy uruchomi≈Ç backend + Postgres bez lokalnej instalacji zale≈ºno≈õci.

```bash
# Skopiuj zmienne (opcjonalnie)
cp .env.docker.example .env

# Uruchom stack (baza + backend)
docker compose up -d --build

# Sprawd≈∫ logi
docker compose logs -f backend

# Przetestuj endpoint
curl http://localhost:5002/api/predictions/energy?days_ahead=3
```

Szczeg√≥≈Çy: zobacz `DOCKER_SETUP.md`.

#### Migrowanie danych do bazy w ≈õrodowisku Docker

Je≈õli chcesz wype≈Çniƒá czystƒÖ bazƒô danymi Garmin/journal:

1. Upewnij siƒô, ≈ºe stack dzia≈Ça:
   ```bash
   docker compose up -d --build
   docker compose ps
   ```
2. Wejd≈∫ do kontenera backend lub uruchom komendƒô bezpo≈õrednio:
   ```bash
   # Pe≈Çna migracja (wszystkie tabele / zakresy)
   docker compose exec backend python run_migration.py --subset all

   # Lub migracje czƒô≈õciowe (przyk≈Çady):
   docker compose exec backend python run_migration.py --subset daily
   docker compose exec backend python run_migration.py --subset sleep
   docker compose exec backend python run_migration.py --subset activities
   docker compose exec backend python run_migration.py --subset journal
   docker compose exec backend python run_migration.py --subset stats  # wyliczenia agregat√≥w minutowych
   ```
3. Walidacja po migracji (przyk≈Çadowe zapytania):
   ```bash
   docker compose exec db psql -U diary_user -d diary -c "SELECT COUNT(*) FROM garmin_daily_summaries;"
   docker compose exec db psql -U diary_user -d diary -c "SELECT COUNT(*) FROM garmin_sleep_sessions;"
   docker compose exec db psql -U diary_user -d diary -c "SELECT COUNT(*) FROM daily_journal;"
   ```
4. Test endpoint√≥w po migracji:
   ```bash
   curl 'http://localhost:5002/api/analytics/enhanced/correlations?days=30' | head
   curl 'http://localhost:5002/api/predictions/energy?days_ahead=3'
   ```

Bezpiecze≈Ñstwo: plik `config.env` nie jest potrzebny wewnƒÖtrz kontenera (zmienne przekazuje compose). Je≈õli dodasz w≈Çasny config ‚Äì nie commituj go do repo.

Przebudowa po zmianach migratora (`scripts/enhanced_migration.py`):
```bash
docker compose build backend
docker compose exec backend python run_migration.py --subset all
```

## üÜï Enhanced Backend API - Zaawansowana Analityka

### üß† Zaawansowane endpointy analityczne:
- `/api/analytics/enhanced/comprehensive` - kompleksowa analiza AI
- `/api/analytics/enhanced/correlations` - korelacje wielowymiarowe
- `/api/analytics/enhanced/clusters` - analiza klastr√≥w zdrowotnych
- `/api/analytics/enhanced/temporal-patterns` - wzorce temporalne
- `/api/analytics/enhanced/recovery` - analiza regeneracji

### üî¨ Specjalistyczne analizy:
- `/api/analytics/sleep/comprehensive` - kompleksowa analiza snu
- `/api/analytics/stress/comprehensive` - analiza wzorc√≥w stresu
- `/api/analytics/activity/comprehensive` - analiza aktywno≈õci

### üîÆ Analityka predykcyjna:
- `/api/predictions/energy` - prognozy poziomu energii
- `/api/predictions/sleep` - prognozy jako≈õci snu
- `/api/predictions/mood` - prognozy nastroju
- `/api/predictions/comprehensive` - kompleksowe prognozy
- `/api/trends/health` - trendy zdrowotne

#### Parametry prognoz (days_ahead)
Ka≈ºdy endpoint predykcyjny przyjmuje parametr zapytania `days_ahead` (alias: `days`) okre≈õlajƒÖcy horyzont prognozy.

Rekomendowane warto≈õci:
- 1‚Äì7 dni: najwy≈ºsza trafno≈õƒá (modele RandomForest + cechy kr√≥tkoterminowe)
- 8‚Äì14 dni: akceptowalne, ale malejƒÖca pewno≈õƒá (confidence spada liniowo)
- >14 dni: mo≈ºliwe, lecz ma≈Ço wiarygodne (generowane wy≈ÇƒÖcznie ekstrapolacyjnie ‚Äì niezalecane)

Ka≈ºda pojedyncza prognoza zawiera:
```json
{
   "date": "2025-10-02",
   "predicted_value": 3.87,
   "confidence": 0.82
}
```
`confidence` maleje wraz z odleg≈Ço≈õciƒÖ dnia w horyzoncie oraz ocenƒÖ jako≈õci modelu (`confidence_level`: high / medium / low / very_low).

Fallback: Gdy zbyt ma≈Ço danych (<30 pe≈Çnych rekord√≥w) model prze≈ÇƒÖcza siƒô na tryb bazowy (trend + ≈õrednia z ostatnich warto≈õci) i zwraca strukturƒô z `confidence_level = very_low`.

### üí° Personalizowane insights:
- `/api/insights/personalized` - spersonalizowane rekomendacje
- `/api/insights/optimization` - optymalizacja metryk zdrowotnych
- `/api/analytics/compare/periods` - por√≥wnania okres√≥w

### üõ†Ô∏è Endpoint administracyjny (operacje ML)
- `POST /api/admin/models/retrain`
   - Usuwa zapisane artefakty modeli (`energy.joblib`, `sleep.joblib`, `mood.joblib`)
   - Modele zostanƒÖ przebudowane leniwie przy nastƒôpnym wywo≈Çaniu endpointu predykcyjnego
   - Opcjonalne body JSON do selektywnej kasacji:
      ```json
      { "models": ["energy", "sleep"] }
      ```
   - Przyk≈Çad odpowiedzi:
      ```json
      {
         "status": "success",
         "removed": ["energy.joblib", "sleep.joblib"],
         "message": "Models deleted; they will be retrained on next prediction request."
      }
      ```

### üîç Wa≈ºne parametry zapyta≈Ñ (query params)
| Obszar | Parametr | Domy≈õlna | Zakres / Uwagi |
|--------|----------|----------|----------------|
| Enhanced analytics | `days` | 90 | 1‚Äì365 |
| Clusters | `clusters` | 3 | 2‚Äì15 (wiƒôksza liczba = wiƒôksze ryzyko szumu) |
| Recovery | `compare` | false | `true` dodaje poprzedni okres trendu |
| Period compare | `period1_days`, `period2_days` | 30 | 1‚Äì365 |
| Period compare | `offset_days` | 30 | odstƒôp miƒôdzy okresami |
| Predictions | `days_ahead` (`days`) | 7 | rekomendowane 1‚Äì14 |
| Insights optimization | `metric` | sleep_quality | dowolna nazwa metryki w zbiorze |

### ‚ôªÔ∏è Trwa≈Ço≈õƒá i retraining modeli
Modele ML sƒÖ zapisywane jako pliki `.joblib` w `Diary-AI-BE/scripts/analytics/models/` i sƒÖ ignorowane przez Git (`.gitignore`).

Strategia:
- Przy starcie: pr√≥ba za≈Çadowania artefaktu; je≈õli niezgodny ‚Äì automatyczne usuniƒôcie i retraining
- Przy b≈Çƒôdzie ≈Çadowania (np. zmiana wersji scikit-learn): wymuszone kasowanie i ponowny trening
- Retraining nastƒôpuje tylko je≈õli model nie istnieje lub jest niekompatybilny (leniwe podej≈õcie)

Confidence logic:
- Globalna jako≈õƒá modelu (`confidence_level`) oparta o R¬≤ (progi: 0.8 / 0.6 / 0.4)
- Per-dzie≈Ñ `confidence` maleje liniowo do min. 0.5 przy ko≈Ñcu horyzontu

### üß™ Przyk≈Çady (curl)
```bash
# 3-dniowa prognoza energii
curl 'http://localhost:5002/api/predictions/energy?days_ahead=3'

# 14-dniowa prognoza snu (g√≥rna granica rekomendacji)
curl 'http://localhost:5002/api/predictions/sleep?days_ahead=14'

# Kompleksowe prognozy (mood + energy + sleep)
curl 'http://localhost:5002/api/predictions/comprehensive?days_ahead=7'

# Por√≥wnanie dw√≥ch okres√≥w (30 dni vs 30 dni z 30-dniowym offsetem)
curl 'http://localhost:5002/api/analytics/compare/periods?period1_days=30&period2_days=30&offset_days=30'

# Recovery z por√≥wnaniem poprzedniego okresu
curl 'http://localhost:5002/api/analytics/enhanced/recovery?days=90&compare=true'

# Kasacja artefakt√≥w modeli (wymuszenie retrainingu)
curl -X POST 'http://localhost:5002/api/admin/models/retrain' \
       -H 'Content-Type: application/json' \
       -d '{"models": ["energy", "sleep"]}'
```

### üíì Monitoring w czasie rzeczywistym:
- **305,354 pomiar√≥w tƒôtna** co minutƒô (24/7)
- **380,672 pomiar√≥w stresu** z kategoryzacjƒÖ
- **268,844 pomiar√≥w czƒôsto≈õci oddechowej**
- **3,362 zdarze≈Ñ snu** z detalowƒÖ analizƒÖ
- **98 pomiar√≥w wagi** z trendem

### üîó Standardowe endpointy API:
- `/api/heart-rate/daily/<date>` - dane tƒôtna dla dnia
- `/api/stress/daily/<date>` - dane stresu z kategoryzacjƒÖ
- `/api/respiratory-rate/daily/<date>` - czƒôsto≈õƒá oddechowa
- `/api/weight/history` - historia wagi
- `/api/sleep/events/<date>` - zdarzenia podczas snu

  - `scripts/services/trends_service.py` ‚Äì zapytania trend√≥w (sen, waga, nastr√≥j)

## \ud83d\ude80 Szybki start

Poni\u017cej znajdziesz najprostszy, przetestowany przep\u0142yw uruchomienia: backend (Postgres + Python API) w Dockerze oraz frontend lokalnie przez npm (u\u017cyteczne podczas developmentu).

1) Uruchom backend + baz\u0119 (Docker)

```bash
# (opcjonalnie) skopiuj przyk\u0142adowe zmienne do pliku .env
cp .env.docker.example .env || true

# Z katalogu root repo uruchom stack (Postgres + backend)
docker compose up -d --build

# Podgl\u0105d log\u00f3w backendu
docker compose logs -f backend
```

Backend domy\u015blnie nas\u0142uchuje na: http://localhost:5002

2) Uruchom frontend lokalnie (oddzielnie, npm)

```bash
# Przejd\u017a do folderu z frontendem React
cd Diary-AI-FE/frontend-react

# Zainstaluj zale\u017cno\u015bci (macOS / zsh)
npm install

# Uruchom frontend dev server
npm start
```

Frontend dev server uruchomi sie domy\u015blnie na: http://localhost:3000 i ma ustawiony "proxy" do backendu `http://localhost:5002` (zdefiniowane w `Diary-AI-FE/frontend-react/package.json`), dzi\u0119ki czemu wywo\u0142ania API z przegl\u0105darki b\u0119d\u0105 kierowane do lokalnego backendu.

3) Alternatywy

- Je\u015bli wolisz uruchomi\u0107 backend lokalnie bez Dockera:

```bash
cp config.env.example config.env
pip install -r Diary-AI-BE/requirements.txt
python scripts/start_enhanced_backend.py
```

- Je\u015bli chcesz serwowa\u0107 frontend statycznie w Dockerze, u\u017cyj skryptu `./start_fresh_docker.sh` (uruchomi tymczasowy nginx by podpi\u0107 zawarto\u015b\u0107 `Diary-AI-FE`), lub zbuduj frontend `npm run build` i zamontuj `build/` do kontenera nginx.

4) Testy zdrowia

```bash
# backend health
curl http://localhost:5002/api/health

# przyk\u0142adowy endpoint (analytics)
curl 'http://localhost:5002/api/predictions/energy?days_ahead=3'
```

Uwagi:
- Port backendu: 5002
- Port frontend dev servera: 3000
- Upewnij si\u0119, \u017ce masz Node.js w wersji zgodnej z `Diary-AI-FE/frontend-react/package.json` (zalecane: Node >=18.18.0 lub >=20)

### Skrypty start/stop (lokalne)
W katalogu g\u0142\u00f3wnym s\u0105 dost\u0119pne uproszczone skrypty:

```bash
./start_all.sh   # uruchamia backend (enhanced) i ewentualne procesy pomocnicze
./stop_all.sh    # zatrzymuje procesy backendu (wariant lokalny)
```

Je\u015bli skrypt nie ma uprawnie\u0144 wykonywalnych:

```bash
chmod +x start_all.sh stop_all.sh
```
‚úÖ **KOMPLETNY SYSTEM Z AI ANALYTICS GOTOWY DO U≈ªYCIA**
- Wszystkie dane Garmin zmigrowane
- Tabele zoptymalizowane i zorganizowane
- Dziennik osobisty zintegrowany
- API i dashboard dzia≈ÇajƒÖ
- **NOWE!** Zaawansowana analityka AI z machine learning
- **NOWE!** Analityka predykcyjna i personalizowane rekomendacje
- **NOWE!** Specjalistyczne modu≈Çy analityczne
- Gotowy do zaawansowanych analiz zdrowotnych

## üîê Bezpiecze≈Ñstwo i publikacja na GitHub

Przed publikacjƒÖ upewnij siƒô ≈ºe:
- NIE commitujesz pliku `config.env` (u≈ºyj `config.env.example` jako szablonu)
- Modele i cache nie zawierajƒÖ danych prywatnych
- Lokalna ≈õcie≈ºka `HEALTH_DATA_PATH` nie wskazuje na prywatny katalog w repo

## üß© Migracja nowych danych (skr√≥t)

Szczeg√≥≈Çowa instrukcja: patrz AI/docs/MANUAL_MIGRATION.md

Najkr√≥tsza ≈õcie≈ºka (Docker):
```bash
cd AI
docker compose up -d --build
# Zamontowa≈Çem ju≈º lokalny HealthData do kontenera backend w docker-compose.yml
# Uruchom pe≈ÇnƒÖ migracjƒô:
docker compose exec backend python - <<'PY'
from enhanced_migration import EnhancedGarminMigrator
m=EnhancedGarminMigrator()
m.migrate_sleep_data(); m.migrate_rhr_data(); m.migrate_daily_summary_data(); m.migrate_heart_rate_data(); m.migrate_respiratory_rate_data(); m.migrate_activities_data()
print('‚úÖ Migracja zako≈Ñczona')
PY
# lub:
# docker compose exec backend python run_migration.py
```

Wyniki sprawdzisz m.in. tak:
```bash
docker compose exec db psql -U diary_user -d diary -c "SELECT COUNT(*) FROM garmin_sleep_sessions;"
docker compose exec db psql -U diary_user -d diary -c "SELECT COUNT(*) FROM garmin_activities;"
```

Je≈õli nie chcesz Dockera:
```bash
cd AI
pip install -r requirements.txt
python setup_migration.py
python enhanced_migration.py
```

## üìÇ Folder `HealthData` ‚Äì co to jest i jak u≈ºywaƒá

`HealthData/` to **lokalny katalog ≈∫r√≥d≈Çowy surowych danych Garmin** (eksport / zrzuty / pliki *.db / CSV), z kt√≥rego migrator pobiera dane i ≈Çaduje je do PostgreSQL.

### Dlaczego nie ma go w repo?
- Zawiera dane wra≈ºliwe / prywatne (tƒôtno, sen, stres, nawyki)
- Pliki binarne i bazy SQLite powiƒôkszy≈Çyby repo i utrudni≈Çy historiƒô
- Dane ≈∫r√≥d≈Çowe powinny byƒá odtwarzalne z prywatnego archiwum u u≈ºytkownika

### Jak wskazaƒá ≈õcie≈ºkƒô?
Ustaw zmiennƒÖ ≈õrodowiskowƒÖ (lub wpis w `config.env`):
```
HEALTH_DATA_PATH=/absolute/path/do/HealthData
```
Je≈õli nie ustawisz ‚Äì migrator spr√≥buje u≈ºyƒá lokalnie `./HealthData` (i zaloguje ostrze≈ºenie gdy nie istnieje).

### Struktura oczekiwana (przyk≈Çad)
```
HealthData/
   Sleep/                    # Pliki snu / JSON / CSV
   RHR/                      # Resting Heart Rate
   Weight/                   # Historia wagi
   DBs/                      # Bazy SQLite (garmin.db, garmin_activities.db itd.)
   Activities/               # (opcjonalnie) pliki aktywno≈õci
   ...
```

### Jak sprawdziƒá czy ≈õcie≈ºka dzia≈Ça
```bash
python Diary-AI-BE/run_migration.py --subset sleep
```
Je≈õli katalog b≈Çƒôdny ‚Äì zobaczysz ostrze≈ºenie o fallbacku lub brak rekord√≥w w tabeli docelowej.

### W ≈õrodowisku Docker
- Domy≈õlnie kontener backend u≈ºywa ≈õcie≈ºki wewnƒôtrznej (je≈õli chcia≈Çby≈õ u≈ºyƒá lokalnych surowych plik√≥w, zamontuj je):
```yaml
   backend:
      volumes:
         - /lokalna/sciezka/HealthData:/app/HealthData:ro
      environment:
         HEALTH_DATA_PATH=/app/HealthData
```

### Dobre praktyki
- Nigdy nie commituj prawdziwego `HealthData/`
- Je≈õli chcesz udostƒôpniƒá strukturƒô, zr√≥b pusty przyk≈Çad typu `HealthData.example/` (bez realnych danych)
- Regularnie archiwizuj ≈∫r√≥d≈Ço (zip + szyfrowanie) poza repo

### Szybka diagnostyka (skrypt w≈Çasny)
Mo≈ºesz stworzyƒá prosty checker (ju≈º masz `simple_check.py` / `test_fixed_migration.py`):
```bash
python simple_check.py
```
Je≈õli wszystko ok ‚Äì zobaczysz ‚úÖ przy podkatalogach (Sleep, RHR, Weight ...)


### Kroki publikacji (je≈õli tworzysz nowe repo)
```bash
git init
git add .
git commit -m "Initial project import"
git branch -M master
# utw√≥rz repo na GitHub (lub u≈ºyj istniejƒÖcego) i dodaj remote:
git remote add origin https://github.com/<twoj-user>/JournAI.git
git push -u origin master
```

### Aktualizacja istniejƒÖcego repo
```bash
git pull --rebase origin master
# wprowad≈∫ zmiany
git add .
git commit -m "<opis zmian>"
git push
```

### Regeneracja ≈õrodowiska po klonowaniu
```bash
cp config.env.example config.env
pip install -r Diary-AI-BE/requirements.txt
python scripts/start_enhanced_backend.py
```

> Je≈õli przypadkowo wypchniesz sekrety: natychmiast je zmie≈Ñ, usu≈Ñ z historii (`git filter-repo` / `git filter-branch`) i force push.

---
*Ostatnia aktualizacja: 2025-09-27 - Enhanced Analytics v1.2.1 (rename repo -> JournAI, instrukcje aktualizacji remote)*
